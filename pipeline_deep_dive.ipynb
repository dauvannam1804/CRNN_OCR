{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deep Dive: CRNN OCR Pipeline\n",
                "\n",
                "This notebook provides a detailed walkthrough of the CRNN OCR pipeline, from data preparation to inference. We will visualize intermediate outputs, including the final feature map before CTC Loss, and explain the CTC Loss calculation in detail.\n",
                "\n",
                "**Core Steps:**\n",
                "1.  **Data Preparation**: Image processing and Label encoding.\n",
                "2.  **Model Forward**: CNN Features -> Reshape -> RNN -> Logits.\n",
                "3.  **Visualization**: Heatmap of the output Logits (Time vs Class).\n",
                "4.  **CTC Loss**: Understanding the inputs and mechanism.\n",
                "5.  **Inference**: Greedy Decoding."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import sys\n",
                "\n",
                "# Add src to path to import modules\n",
                "sys.path.append('src')\n",
                "\n",
                "from dataset import OCRDataset, get_vocab\n",
                "from model import CRNN\n",
                "from utils import decode_greedy\n",
                "\n",
                "# Configuration\n",
                "DATA_ROOT = 'data'\n",
                "CHECKPOINT_PATH = 'checkpoints/best_model.pth'\n",
                "IMG_HEIGHT = 32\n",
                "IMG_WIDTH = 100 # Consistent with training\n",
                "DEVICE = torch.device('cpu') # Use CPU for easier debugging/visualization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Preparation (Train/Val Split & Encoding)\n",
                "\n",
                "**Goal**: Prepare the raw image and label for the network.\n",
                "\n",
                "-   **Image**: Resize to (32, 100), Convert to Grayscale (1 channel), Normalize (-1, 1).\n",
                "-   **Label**: Convert characters to integers using a Vocabulary Mapping.\n",
                "\n",
                "The `OCRDataset` class handles this via `__getitem__`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Get Vocabulary\n",
                "vocab = get_vocab(os.path.join(DATA_ROOT, 'trainset'), os.path.join(DATA_ROOT, 'testset'))\n",
                "print(f\"Vocabulary ({len(vocab)}): {vocab}\")\n",
                "\n",
                "# 2. Initialize Dataset\n",
                "train_ds = OCRDataset(os.path.join(DATA_ROOT, 'trainset'), vocab, height=IMG_HEIGHT, width=IMG_WIDTH)\n",
                "idx2char = train_ds.idx2char\n",
                "\n",
                "# 3. Get a sample\n",
                "idx = 0  # Change this to see different samples\n",
                "image_tensor, label_tensor, label_len = train_ds[idx]\n",
                "\n",
                "print(f\"\\nSample Index: {idx}\")\n",
                "print(f\"Image Shape: {image_tensor.shape} (Channel, Height, Width)\")\n",
                "print(f\"Label Tensor: {label_tensor}\")\n",
                "print(f\"Label Length: {label_len.item()}\")\n",
                "print(f\"Encoded Label string: {''.join([idx2char[i.item()] for i in label_tensor])}\")\n",
                "\n",
                "# Visualize\n",
                "plt.imshow(image_tensor.squeeze(), cmap='gray')\n",
                "plt.title(f\"Preprocessed Image Input (Label: {''.join([idx2char[i.item()] for i in label_tensor])})\")\n",
                "plt.axis('on')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Forward Pass & Visualization\n",
                "\n",
                "We will load the trained model and pass this single image through it. However, instead of just getting the final output, we will inspect the shapes at each major block.\n",
                "\n",
                "### Pipeline Stages:\n",
                "1.  **CNN Backbone**: Extracts visual features. Input: `(B, 1, 32, 100)` -> Output: `(B, 512, 1, 26)`.\n",
                "    *   *Note*: The Height became 1 (vertical Information compressed), Width became 26 (Horizontal/Time information compressed but preserved).\n",
                "    *   The width 26 corresponds to the number of \"Timesteps\" (T).\n",
                "2.  **Reshape**: Prepare for RNN sequence. `(B, 512, 1, 26)` -> `(26, B, 512)` (Seq, Batch, Feature).\n",
                "3.  **RNN Head**: Contextual sequence modeling. Output: `(26, B, Hidden*2)`.\n",
                "4.  **Linear Projection**: Map to classes. Output: `(26, B, NumClasses)`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Model\n",
                "n_class = len(vocab) + 1 # +1 for blank\n",
                "model = CRNN(IMG_HEIGHT, 1, n_class, 256).to(DEVICE)\n",
                "model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n",
                "model.eval()\n",
                "\n",
                "# Prepare Input Batch (Batch Size = 1)\n",
                "input_batch = image_tensor.unsqueeze(0).to(DEVICE) # [1, 1, 32, 100]\n",
                "\n",
                "print(\"---- Forward Pass Tracking ----\")\n",
                "print(f\"1. Input: {input_batch.shape}\")\n",
                "\n",
                "# A. CNN Pass\n",
                "with torch.no_grad():\n",
                "    cnn_features = model.cnn(input_batch)\n",
                "print(f\"2. CNN Output features: {cnn_features.shape} (Batch, Channel, Height, Width)\")\n",
                "\n",
                "# B. Pre-RNN Reshape\n",
                "b, c, h, w = cnn_features.size()\n",
                "assert h == 1, \"Height must be 1\"\n",
                "reshaped_features = cnn_features.squeeze(2) # [B, C, W]\n",
                "reshaped_features = reshaped_features.permute(2, 0, 1) # [W, B, C] -> [Time, Batch, Feature]\n",
                "print(f\"3. Input to RNN (Permuted): {reshaped_features.shape} (Time, Batch, Feature)\")\n",
                "\n",
                "# C. RNN Pass\n",
                "with torch.no_grad():\n",
                "    rnn_output = model.rnn(reshaped_features)\n",
                "print(f\"4. Model Output (Logits): {rnn_output.shape} (Time, Batch, NumClasses)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualizing the Output (Features/Logits)\n",
                "\n",
                "This is the requested plot: **The feature map / output of the final layer before CTC Loss**.\n",
                "\n",
                "We have a matrix of shape `[Time (26), Class (39)]`. Each column represents a timestep, and each row represents the probability of a specific character at that timestep.\n",
                "\n",
                "**How to read the plot:**\n",
                "-   **Y-axis**: The Characters (Classes). Index 0 is usually `Blank`.\n",
                "-   **X-axis**: Time steps (0 to 25).\n",
                "-   **Color**: Probability (Softmax) of that character at that time.\n",
                "-   **Bright Spots**: The model is confident that character is present at that rough horizontal location in the image."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply Softmax to convert Logits to Probabilities for visualizing\n",
                "probs = rnn_output.softmax(dim=2).squeeze(1).numpy() # [Time, Classes]\n",
                "probs = probs.T # Transpose for plotting: [Classes, Time]\n",
                "\n",
                "# Setup Plot\n",
                "plt.figure(figsize=(15, 8))\n",
                "plt.imshow(probs, aspect='auto', cmap='viridis', interpolation='nearest')\n",
                "\n",
                "# Annotate Y-axis with characters\n",
                "tick_locs = np.arange(n_class)\n",
                "tick_labels = ['<BLANK>'] + [idx2char[i] for i in range(1, n_class)]\n",
                "plt.yticks(tick_locs, tick_labels, fontsize=8)\n",
                "\n",
                "plt.xlabel(\"Timesteps (Output Width)\")\n",
                "plt.ylabel(\"Classes (Characters)\")\n",
                "plt.title(f\"Probability Heatmap (Model Output)\\nTrue Label: {''.join([idx2char[i.item()] for i in label_tensor])}\")\n",
                "plt.colorbar(label=\"Probability\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. CTC Loss Calculation (Deep Dive)\n",
                "\n",
                "**The Problem**: Our output has 26 timesteps, but our label \"cat\" only has 3 characters. We don't know *exactly* which timestep aligns to 'c', 'a', or 't'.\n",
                "\n",
                "**CTC Solution**: Sum the probabilities of **ALL valid paths** that decode to \"cat\".\n",
                "\n",
                "**Valid Paths**: To transform a path to a label:\n",
                "1.  Collapse repeated characters: `cc` -> `c`\n",
                "2.  Remove blanks: `_` -> (nothing)\n",
                "3.  Example paths for \"cat\":\n",
                "    -   `c_a_t`\n",
                "    -   `cc_at`\n",
                "    -   `_caat`\n",
                "\n",
                "**The Formula**:\n",
                "Loss = -log(P(Label | Input))\n",
                "Where P(Label | Input) = Sum(Product of probabilities for each step in a valid path).\n",
                "\n",
                "**Inputs to PyTorch `ctc_loss`**:\n",
                "1.  `log_probs`: `(Time, Batch, Classes)` [Log Softmax values]\n",
                "2.  `targets`: Flat 1D tensor of all labels concatenated.\n",
                "3.  `input_lengths`: Vector size `(Batch,)` containing actual length of each Time sequence (usually all 26 here).\n",
                "4.  `target_lengths`: Vector size `(Batch,)` containing length of each label (e.g., 3 for \"cat\")."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
                "\n",
                "# 1. Prepare Log Probs (Required by PyTorch CTC)\n",
                "log_probs = rnn_output.log_softmax(2) # [Time, Batch, Class]\n",
                "print(f\"Log Probs Shape: {log_probs.shape}\")\n",
                "\n",
                "# 2. Prepare Targets\n",
                "targets = label_tensor # [Label Len]\n",
                "print(f\"Targets: {targets}\")\n",
                "\n",
                "# 3. Input Lengths (The 'T' dimension)\n",
                "T = log_probs.size(0)\n",
                "input_lengths = torch.LongTensor([T]) # [26]\n",
                "print(f\"Input Lengths: {input_lengths}\")\n",
                "\n",
                "# 4. Target Lengths\n",
                "target_lengths = label_len # [Len]\n",
                "print(f\"Target Lengths: {target_lengths}\")\n",
                "\n",
                "# Calculate Loss\n",
                "loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
                "print(f\"\\nCalculated CTC Loss: {loss.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inference & Greedy Decoding\n",
                "\n",
                "How do we turn the heatmap back into text?\n",
                "\n",
                "**Greedy Decoding**:\n",
                "1.  Take the index with the maximum probability at each timestep (`argmax`).\n",
                "2.  This gives a \"raw path\".\n",
                "3.  **CTC Collapse Rule**: Remove repeated adjacent duplicates, THEN remove blanks.\n",
                "\n",
                "*Example*:\n",
                "-   Raw Path Indices: `[0, 3, 3, 0, 1, 1, 0, 20] `  (Assuming 0=Blank, 3=c, 1=a, 20=t)\n",
                "-   Raw String: `_cc_aa_t`\n",
                "-   Collapse Repeats (Group): `_`, `c`, `_`, `a`, `_`, `t`\n",
                "-   Remove Blanks: `c`, `a`, `t` -> \"cat\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Argmax path\n",
                "preds_indices = rnn_output.argmax(2).squeeze(1).tolist() # [Time]\n",
                "print(f\"Raw Path Indices (Best per timestep): {preds_indices}\")\n",
                "\n",
                "# 2. Convert to Chars (for visualization)\n",
                "# Note: We handle 0 as <BLANK>\n",
                "raw_chars = []\n",
                "for idx in preds_indices:\n",
                "    if idx == 0:\n",
                "        raw_chars.append('-') # Using '-' for blank visualization\n",
                "    else:\n",
                "        raw_chars.append(idx2char[idx])\n",
                "print(f\"Raw Path String: {''.join(raw_chars)}\")\n",
                "\n",
                "# 3. Decoding Logic (Included in utils.decode_greedy)\n",
                "decoded_text = decode_greedy(rnn_output, idx2char)[0]\n",
                "print(f\"\\nFinal Decoded Text: {decoded_text}\")\n",
                "print(f\"Ground Truth: {''.join([idx2char[i.item()] for i in label_tensor])}\")\n",
                "\n",
                "if decoded_text == ''.join([idx2char[i.item()] for i in label_tensor]):\n",
                "    print(\"\\nSUCCESS: Prediction matches Ground Truth!\")\n",
                "else:\n",
                "    print(\"\\nMISMATCH: Prediction incorrect.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}